{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "import dotenv\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv('.env') \n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "client = Groq(api_key=groq_api_key)\n",
    "MODEL = 'llama-3.3-70b-versatile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo vector database với FAISS\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"rag_aio\",\n",
    "            \"description\": \"Trả lời câu hỏi dựa trên dữ liệu từ tài liệu AIO PDF (ví dụ: AI Agent là gì)\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"question\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Câu hỏi của người dùng cần trả lời\"\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"question\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"rag_billionares\",\n",
    "            \"description\": \"Trả lời câu hỏi dựa trên dữ liệu từ tài liệu Billionares PDF (ví dụ: Có bao nhiêu tỷ phú ở Mỹ)\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"question\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Câu hỏi của người dùng về tỷ phú\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"question\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"rag_economic\",\n",
    "            \"description\": \"Trả lời câu hỏi dựa trên dữ liệu từ tài liệu Economic PDF (về kinh tế và tài chính)\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"question\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Câu hỏi của người dùng về kinh tế\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"question\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "# rerank search results\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def retrieve_and_re_rank_advanced(vector_db, query, k=10):\n",
    "    # Lấy kết quả từ Vector Database\n",
    "    docs_with_scores = vector_db.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    # Chuẩn bị dữ liệu cho Cross-Encoder\n",
    "    doc_texts = [doc.page_content for doc, _ in docs_with_scores]\n",
    "    pairs = [[query, doc] for doc in doc_texts]\n",
    "    \n",
    "    # Dùng Cross-Encoder để đánh giá lại\n",
    "    rerank_scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Sắp xếp lại theo score mới\n",
    "    ranked_docs = sorted(zip(doc_texts, rerank_scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    results = [doc for doc, _ in ranked_docs]\n",
    "    scores = [score for _, score in ranked_docs]\n",
    "    \n",
    "    return results, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_aio(question: str):\n",
    "    vector_db = FAISS.load_local('/home/thangcn/Downloads/datn/faiss_db/pdf_aio', embeddings, allow_dangerous_deserialization=True)\n",
    "    retrieved_docs, scores = retrieve_and_re_rank_advanced(vector_db, question)\n",
    "        # Gửi yêu cầu đến mô hình Groq\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that utilizes retrieved information.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{retrieved_docs[0]}\\n\\nQuestion: {question}\"},\n",
    "        ],\n",
    "        model= MODEL,\n",
    "    )\n",
    "    print(chat_completion.choices[0].message.content)\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def rag_billionares(question: str):\n",
    "    vector_db = FAISS.load_local('/home/thangcn/Downloads/datn/faiss_db/pdf_billionares', embeddings, allow_dangerous_deserialization=True)\n",
    "    retrieved_docs, scores = retrieve_and_re_rank_advanced(vector_db, question)\n",
    "        # Gửi yêu cầu đến mô hình Groq\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that utilizes retrieved information.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{retrieved_docs[0]}\\n\\nQuestion: {question}\"},\n",
    "        ],\n",
    "        model= MODEL,\n",
    "    )\n",
    "    print(chat_completion.choices[0].message.content)\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def rag_economic(question: str):\n",
    "    vector_db = FAISS.load_local('/home/thangcn/Downloads/datn/faiss_db/pdf_economic', embeddings, allow_dangerous_deserialization=True)\n",
    "    retrieved_docs, scores = retrieve_and_re_rank_advanced(vector_db, question)\n",
    "        # Gửi yêu cầu đến mô hình Groq\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that utilizes retrieved information.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{retrieved_docs[0]}\\n\\nQuestion: {question}\"},\n",
    "        ],\n",
    "        model= MODEL,\n",
    "    )\n",
    "    print(chat_completion.choices[0].message.content)\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conversation(user_prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a function calling LLM that uses the data extracted from the functions to answer questions around mutual funds, UPI transactions, health insurance policies, and cash loans.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096\n",
    "    )\n",
    "\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    # ChatCompletionMessage(\n",
    "    #     content=None, \n",
    "    #     role='assistant', \n",
    "    #     function_call=None, \n",
    "    #     reasoning=None, \n",
    "    #     tool_calls=[\n",
    "    #         ChatCompletionMessageToolCall(\n",
    "    #             id='call_zd18', \n",
    "    #             function=Function(arguments='{\"transaction_id\":\"TX123\"}', name='upi'), \n",
    "    #             type='function'\n",
    "    #         )\n",
    "    #     ]\n",
    "    # )\n",
    "    # print(response_message)\n",
    "    # print('-' * 100)\n",
    "    if tool_calls:\n",
    "        # try:\n",
    "        available_functions = {\n",
    "            \"rag_aio\": rag_aio,\n",
    "            \"rag_billionares\": rag_billionares,\n",
    "            \"rag_economic\": rag_economic\n",
    "        }\n",
    "\n",
    "        messages.append(response_message)\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            print(function_name)\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(**function_args)\n",
    "            \n",
    "            return function_response\n",
    "        #     messages.append({\n",
    "        #         \"tool_call_id\": tool_call.id,\n",
    "        #         \"role\": \"tool\",\n",
    "        #         \"name\": function_name,\n",
    "        #         \"content\": function_response,\n",
    "        #     })\n",
    "            \n",
    "        # second_response = client.chat.completions.create(\n",
    "        #     model=MODEL,\n",
    "        #     messages=messages\n",
    "        # )\n",
    "\n",
    "        # final_response = second_response.choices[0].message.content\n",
    "        # except Exception as e:\n",
    "        #     return \"Sorry I'm not able to answer that question\"\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            max_tokens=4096\n",
    "        )\n",
    "        messages.append(response_message)\n",
    "        final_response = response.choices[0].message.content\n",
    "\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_aio\n",
      "AI Agent (hay Agent) là một thành phần trong lĩnh vực Trí tuệ Nhân tạo (AI) cho phép chatbot thực hiện nhiều công việc hơn, không chỉ giới hạn trong việc trò chuyện và trả lời câu hỏi. Agent có thể kết nối với các nguồn dữ liệu, thực hiện các tác vụ phức tạp, tìm kiếm thông tin mới, thay đổi dữ liệu hiện có, và thực hiện các công việc tự động hóa khác.\n",
      "\n",
      "Agent là một bước tiến quan trọng trong việc phát triển các hệ thống AI, vì nó cho phép chatbot trở thành một công cụ hữu ích hơn, linh hoạt hơn và có thể thực hiện nhiều nhiệm vụ khác nhau. Với Agent, chatbot có thể trở thành một trợ lý ảo thực sự, giúp người dùng thực hiện các công việc hàng ngày và cung cấp thông tin một cách hiệu quả hơn.\n"
     ]
    }
   ],
   "source": [
    "input = \"AI Agent là gì?\"\n",
    "response = run_conversation(input)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
